{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dc7179",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-12T13:54:40.083171Z",
     "iopub.status.busy": "2026-01-12T13:54:40.082883Z",
     "iopub.status.idle": "2026-01-12T13:54:41.009348Z",
     "shell.execute_reply": "2026-01-12T13:54:41.008543Z"
    },
    "papermill": {
     "duration": 0.931747,
     "end_time": "2026-01-12T13:54:41.010862",
     "exception": false,
     "start_time": "2026-01-12T13:54:40.079115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sex</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>branch_code</th>\n",
       "      <th>occupation_code</th>\n",
       "      <th>occupation_category_code</th>\n",
       "      <th>P5DA</th>\n",
       "      <th>RIBP</th>\n",
       "      <th>8NN1</th>\n",
       "      <th>7POT</th>\n",
       "      <th>...</th>\n",
       "      <th>K6QO</th>\n",
       "      <th>QBOL</th>\n",
       "      <th>JWFN</th>\n",
       "      <th>JZ9D</th>\n",
       "      <th>J9JW</th>\n",
       "      <th>GHYX</th>\n",
       "      <th>ECY3</th>\n",
       "      <th>join_year</th>\n",
       "      <th>age</th>\n",
       "      <th>total_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>1X1H</td>\n",
       "      <td>2A7I</td>\n",
       "      <td>T4MS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>UAOD</td>\n",
       "      <td>2A7I</td>\n",
       "      <td>T4MS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>748L</td>\n",
       "      <td>QZYX</td>\n",
       "      <td>90QI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>1X1H</td>\n",
       "      <td>BP09</td>\n",
       "      <td>56SI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>748L</td>\n",
       "      <td>NO3L</td>\n",
       "      <td>T4MS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 sex marital_status branch_code occupation_code  \\\n",
       "0           0   F              M        1X1H            2A7I   \n",
       "1           1   F              M        UAOD            2A7I   \n",
       "2           2   M              U        748L            QZYX   \n",
       "3           3   M              M        1X1H            BP09   \n",
       "4           4   M              M        748L            NO3L   \n",
       "\n",
       "  occupation_category_code  P5DA  RIBP  8NN1  7POT  ...  K6QO  QBOL  JWFN  \\\n",
       "0                     T4MS     0     0     0     0  ...     1     0     0   \n",
       "1                     T4MS     0     0     0     0  ...     1     0     0   \n",
       "2                     90QI     0     0     0     0  ...     0     0     0   \n",
       "3                     56SI     0     0     0     0  ...     1     0     0   \n",
       "4                     T4MS     0     0     0     0  ...     0     0     0   \n",
       "\n",
       "   JZ9D  J9JW  GHYX  ECY3  join_year  age  total_products  \n",
       "0     0     0     0     0     2019.0   33               2  \n",
       "1     0     0     0     0     2019.0   39               2  \n",
       "2     0     0     0     1     2013.0   29               3  \n",
       "3     0     0     0     0     2019.0   30               2  \n",
       "4     1     1     0     0     2019.0   30               2  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/insurance-recommendation-challenge/train_.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8d6fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T13:54:41.016199Z",
     "iopub.status.busy": "2026-01-12T13:54:41.015959Z",
     "iopub.status.idle": "2026-01-12T13:54:42.375511Z",
     "shell.execute_reply": "2026-01-12T13:54:42.374738Z"
    },
    "papermill": {
     "duration": 1.364212,
     "end_time": "2026-01-12T13:54:42.377225",
     "exception": false,
     "start_time": "2026-01-12T13:54:41.013013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Liste des colonnes produits\n",
    "PRODUCT_COLUMNS = [\n",
    "    'P5DA', 'RIBP', '8NN1', '7POT', '66FJ', 'GYSR', 'SOP4', 'RVSZ',\n",
    "    'PYUQ', 'LJR9', 'N2MW', 'AHXO', 'BSTQ', 'FM3X', 'K6QO', 'QBOL',\n",
    "    'JWFN', 'JZ9D', 'J9JW', 'GHYX', 'ECY3'\n",
    "]\n",
    "\n",
    "# Colonnes catégorielles à encoder\n",
    "CATEGORICAL_COLUMNS = ['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code']\n",
    "\n",
    "# Colonnes numériques à normaliser\n",
    "NUMERICAL_COLUMNS = ['join_year', 'age', 'total_products']\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Preprocess the input DataFrame by standardizing date formats, extracting year information,\n",
    "    calculating age, and dropping unnecessary columns.\n",
    "    :param df: Input DataFrame with columns 'ID', 'join_date', 'birth_year', 'sex', etc.\n",
    "    :return: Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'join_date' to datetime, standardize\n",
    "    df['join_date'] = pd.to_datetime(df['join_date'], errors='coerce')\n",
    "\n",
    "    # Standardize 'sex' to uppercase\n",
    "    df['sex'] = df['sex'].str.upper()\n",
    "\n",
    "    # Extract 'join_year' from 'join_date' and calculate 'age'\n",
    "    df['join_year'] = df['join_date'].dt.year\n",
    "    current_year = 2020\n",
    "    df['age'] = current_year - df['birth_year']\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    cols = list(df.columns)\n",
    "    cols = [col for col in cols if col not in ['ID', 'join_date', 'birth_year']]\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_for_training(df, fit_encoders=True, encoder=None, scaler=None):\n",
    "    \"\"\"\n",
    "    Prépare les données pour l'entraînement d'un modèle de classification multi-label en utilisant One-Hot Encoding\n",
    "    et la normalisation des variables numériques.\n",
    "\n",
    "    Etapes:\n",
    "    1. Supprimer la colonne 'Unnamed: 0' si elle est présente.\n",
    "\n",
    "    2. Gérer les valeurs manquantes:\n",
    "        - Pour les colonnes catégorielles, remplacer les NaN par 'UNKNOWN'.\n",
    "        - Pour les colonnes numériques, remplacer les NaN par la médiane de la colonne.\n",
    "\n",
    "    3. Séparer les colonnes catégorielles et numériques.\n",
    "\n",
    "    4. Appliquer le One-Hot Encoding aux colonnes catégorielles.\n",
    "\n",
    "    5. Normaliser les colonnes numériques avec StandardScaler.\n",
    "\n",
    "    :param df: DataFrame préprocessé\n",
    "    :param fit_encoders: Si True, ajuste les encoders sur les données\n",
    "    :param encoder: OneHotEncoder déjà ajusté (pour le mode test)\n",
    "    :param scaler: Scaler déjà ajusté (pour le mode test)\n",
    "    :return: X (features), y (labels), encoder, scaler\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Supprimer 'Unnamed: 0' si présent\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    # 2. Gestion des valeurs manquantes\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('UNKNOWN').astype(str)\n",
    "\n",
    "    for col in NUMERICAL_COLUMNS:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # 3. Séparer colonnes catégorielles et numériques\n",
    "    cat_cols_present = [col for col in CATEGORICAL_COLUMNS if col in df.columns]\n",
    "    num_cols_present = [col for col in NUMERICAL_COLUMNS if col in df.columns]\n",
    "\n",
    "    # 4. One-Hot Encoding des variables catégorielles\n",
    "    if fit_encoders:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        cat_encoded = encoder.fit_transform(df[cat_cols_present])\n",
    "    else:\n",
    "        cat_encoded = encoder.transform(df[cat_cols_present])\n",
    "\n",
    "    cat_encoded_df = pd.DataFrame(\n",
    "        cat_encoded,\n",
    "        columns=encoder.get_feature_names_out(cat_cols_present),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # 5. Normalisation des variables numériques\n",
    "    if fit_encoders:\n",
    "        scaler = StandardScaler()\n",
    "        num_scaled = scaler.fit_transform(df[num_cols_present])\n",
    "    else:\n",
    "        num_scaled = scaler.transform(df[num_cols_present])\n",
    "\n",
    "    num_scaled_df = pd.DataFrame(\n",
    "        num_scaled,\n",
    "        columns=num_cols_present,\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # 6. Combiner features\n",
    "    X = pd.concat([num_scaled_df, cat_encoded_df], axis=1)\n",
    "\n",
    "    # 7. Extraire labels\n",
    "    label_columns = [col for col in PRODUCT_COLUMNS if col in df.columns]\n",
    "    y = df[label_columns] if label_columns else None\n",
    "\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d70063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T13:54:42.382323Z",
     "iopub.status.busy": "2026-01-12T13:54:42.381977Z",
     "iopub.status.idle": "2026-01-12T13:54:42.488641Z",
     "shell.execute_reply": "2026-01-12T13:54:42.488043Z"
    },
    "papermill": {
     "duration": 0.111212,
     "end_time": "2026-01-12T13:54:42.490419",
     "exception": false,
     "start_time": "2026-01-12T13:54:42.379207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Sépare le DataFrame en ensembles d'entraînement et de test.\n",
    "\n",
    "    :param df: DataFrame complet\n",
    "    :param test_size: Proportion des données pour le test (défaut: 0.2)\n",
    "    :param random_state: Seed pour la reproductibilité (défaut: 42)\n",
    "    :return: df_train, df_test\n",
    "    \"\"\"\n",
    "    df_train, df_test = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_train, df_test = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398190ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T13:54:42.496322Z",
     "iopub.status.busy": "2026-01-12T13:54:42.495559Z",
     "iopub.status.idle": "2026-01-12T13:54:42.710126Z",
     "shell.execute_reply": "2026-01-12T13:54:42.709499Z"
    },
    "papermill": {
     "duration": 0.219391,
     "end_time": "2026-01-12T13:54:42.711994",
     "exception": false,
     "start_time": "2026-01-12T13:54:42.492603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Préparer pour l'entraînement\n",
    "X_train, y_train, encoder, scaler = prepare_for_training(df_train, fit_encoders=True)\n",
    "\n",
    "# Pour les données de test (utiliser les mêmes encoders)\n",
    "X_test, y_test, _, _ = prepare_for_training(df_test, fit_encoders=False, encoder=encoder, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6331e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T13:54:42.717606Z",
     "iopub.status.busy": "2026-01-12T13:54:42.716941Z",
     "iopub.status.idle": "2026-01-12T13:54:42.737900Z",
     "shell.execute_reply": "2026-01-12T13:54:42.737204Z"
    },
    "papermill": {
     "duration": 0.025101,
     "end_time": "2026-01-12T13:54:42.739246",
     "exception": false,
     "start_time": "2026-01-12T13:54:42.714145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>join_year</th>\n",
       "      <th>age</th>\n",
       "      <th>total_products</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>marital_status_D</th>\n",
       "      <th>marital_status_M</th>\n",
       "      <th>marital_status_P</th>\n",
       "      <th>marital_status_R</th>\n",
       "      <th>marital_status_S</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation_code_ZCQR</th>\n",
       "      <th>occupation_code_ZHC2</th>\n",
       "      <th>occupation_code_ZKQ3</th>\n",
       "      <th>occupation_code_ZWPL</th>\n",
       "      <th>occupation_category_code_56SI</th>\n",
       "      <th>occupation_category_code_90QI</th>\n",
       "      <th>occupation_category_code_AHH5</th>\n",
       "      <th>occupation_category_code_JD7X</th>\n",
       "      <th>occupation_category_code_L44T</th>\n",
       "      <th>occupation_category_code_T4MS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.668407</td>\n",
       "      <td>0.379659</td>\n",
       "      <td>-0.464141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145822</td>\n",
       "      <td>1.133025</td>\n",
       "      <td>6.143558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145822</td>\n",
       "      <td>-0.373706</td>\n",
       "      <td>1.187784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.145822</td>\n",
       "      <td>-1.342319</td>\n",
       "      <td>-0.464141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.145822</td>\n",
       "      <td>-1.449943</td>\n",
       "      <td>-0.464141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   join_year       age  total_products  sex_F  sex_M  marital_status_D  \\\n",
       "0   0.668407  0.379659       -0.464141    0.0    1.0               0.0   \n",
       "1   0.145822  1.133025        6.143558    0.0    1.0               0.0   \n",
       "2   0.145822 -0.373706        1.187784    0.0    1.0               0.0   \n",
       "3   0.145822 -1.342319       -0.464141    0.0    1.0               0.0   \n",
       "4   0.145822 -1.449943       -0.464141    0.0    1.0               0.0   \n",
       "\n",
       "   marital_status_M  marital_status_P  marital_status_R  marital_status_S  \\\n",
       "0               1.0               0.0               0.0               0.0   \n",
       "1               1.0               0.0               0.0               0.0   \n",
       "2               1.0               0.0               0.0               0.0   \n",
       "3               1.0               0.0               0.0               0.0   \n",
       "4               1.0               0.0               0.0               0.0   \n",
       "\n",
       "   ...  occupation_code_ZCQR  occupation_code_ZHC2  occupation_code_ZKQ3  \\\n",
       "0  ...                   0.0                   0.0                   0.0   \n",
       "1  ...                   0.0                   0.0                   0.0   \n",
       "2  ...                   0.0                   0.0                   0.0   \n",
       "3  ...                   0.0                   0.0                   0.0   \n",
       "4  ...                   0.0                   0.0                   0.0   \n",
       "\n",
       "   occupation_code_ZWPL  occupation_category_code_56SI  \\\n",
       "0                   0.0                            0.0   \n",
       "1                   0.0                            0.0   \n",
       "2                   0.0                            0.0   \n",
       "3                   0.0                            1.0   \n",
       "4                   0.0                            0.0   \n",
       "\n",
       "   occupation_category_code_90QI  occupation_category_code_AHH5  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            1.0                            0.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   occupation_category_code_JD7X  occupation_category_code_L44T  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            1.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   occupation_category_code_T4MS  \n",
       "0                            1.0  \n",
       "1                            0.0  \n",
       "2                            0.0  \n",
       "3                            0.0  \n",
       "4                            1.0  \n",
       "\n",
       "[5 rows x 255 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949b011f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T13:54:42.745719Z",
     "iopub.status.busy": "2026-01-12T13:54:42.745067Z",
     "iopub.status.idle": "2026-01-12T13:57:29.608154Z",
     "shell.execute_reply": "2026-01-12T13:57:29.607311Z"
    },
    "papermill": {
     "duration": 166.868445,
     "end_time": "2026-01-12T13:57:29.609813",
     "exception": false,
     "start_time": "2026-01-12T13:54:42.741368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 13:54:45.298310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768226085.475731      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768226085.524997      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768226085.939238      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768226085.939285      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768226085.939287      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768226085.939290      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      " ZIMNAT INSURANCE - CLASSIFICATION MULTI-LABEL\n",
      "------------------------------------------------------------\n",
      "\n",
      " Entraînement Random Forest...\n",
      "\n",
      "--------------------------------------------------\n",
      "Résultats pour Random Forest\n",
      "--------------------------------------------------\n",
      "F1 Score (micro):     0.6917\n",
      "F1 Score (macro):     0.4177\n",
      "F1 Score (weighted):  0.7917\n",
      "Precision (micro):    0.5858\n",
      "Recall (micro):       0.8443\n",
      "Hamming Loss:         0.0812\n",
      "Subset Accuracy:      0.4891\n",
      "Modèle sauvegardé: /kaggle/working/models/random_forest.pkl\n",
      "\n",
      " Entraînement XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:40] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:55:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Résultats pour XGBoost\n",
      "--------------------------------------------------\n",
      "F1 Score (micro):     0.8465\n",
      "F1 Score (macro):     0.4513\n",
      "F1 Score (weighted):  0.8248\n",
      "Precision (micro):    0.8630\n",
      "Recall (micro):       0.8305\n",
      "Hamming Loss:         0.0325\n",
      "Subset Accuracy:      0.6954\n",
      "Modèle sauvegardé: /kaggle/working/models/xgboost.pkl\n",
      "\n",
      " Entraînement CatBoost...\n",
      "\n",
      "--------------------------------------------------\n",
      "Résultats pour CatBoost\n",
      "--------------------------------------------------\n",
      "F1 Score (micro):     0.7136\n",
      "F1 Score (macro):     0.4155\n",
      "F1 Score (weighted):  0.7910\n",
      "Precision (micro):    0.6226\n",
      "Recall (micro):       0.8358\n",
      "Hamming Loss:         0.0723\n",
      "Subset Accuracy:      0.4737\n",
      "Modèle sauvegardé: /kaggle/working/models/catboost.pkl\n",
      "\n",
      " Entraînement Deep Learning (TensorFlow)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1768226201.596237      24 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14497 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">693</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │           \u001b[38;5;34m693\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,253</span> (434.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m111,253\u001b[0m (434.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,357</span> (431.08 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,357\u001b[0m (431.08 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768226206.016805     534 service.cc:152] XLA service 0x345e6cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1768226206.016847     534 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1768226206.596232     534 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 62/365\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0895 - auc: 0.6537 - loss: 0.6243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1768226209.674015     534 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.3624 - auc: 0.8384 - loss: 0.3604 - val_accuracy: 0.8325 - val_auc: 0.9719 - val_loss: 0.1137 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7150 - auc: 0.9645 - loss: 0.1239 - val_accuracy: 0.8598 - val_auc: 0.9781 - val_loss: 0.0997 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7556 - auc: 0.9715 - loss: 0.1130 - val_accuracy: 0.8589 - val_auc: 0.9798 - val_loss: 0.0964 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7756 - auc: 0.9737 - loss: 0.1106 - val_accuracy: 0.8584 - val_auc: 0.9800 - val_loss: 0.0952 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8044 - auc: 0.9755 - loss: 0.1065 - val_accuracy: 0.8481 - val_auc: 0.9800 - val_loss: 0.0947 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8060 - auc: 0.9760 - loss: 0.1058 - val_accuracy: 0.8481 - val_auc: 0.9812 - val_loss: 0.0931 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8061 - auc: 0.9779 - loss: 0.1022 - val_accuracy: 0.8517 - val_auc: 0.9815 - val_loss: 0.0931 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8130 - auc: 0.9785 - loss: 0.1001 - val_accuracy: 0.8378 - val_auc: 0.9812 - val_loss: 0.0929 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8089 - auc: 0.9790 - loss: 0.0997 - val_accuracy: 0.8389 - val_auc: 0.9818 - val_loss: 0.0931 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8124 - auc: 0.9798 - loss: 0.0989 - val_accuracy: 0.8443 - val_auc: 0.9819 - val_loss: 0.0921 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8075 - auc: 0.9792 - loss: 0.0999 - val_accuracy: 0.8359 - val_auc: 0.9812 - val_loss: 0.0925 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8086 - auc: 0.9803 - loss: 0.0968 - val_accuracy: 0.8030 - val_auc: 0.9817 - val_loss: 0.0924 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7971 - auc: 0.9804 - loss: 0.0965 - val_accuracy: 0.8495 - val_auc: 0.9814 - val_loss: 0.0923 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8055 - auc: 0.9806 - loss: 0.0959 - val_accuracy: 0.8368 - val_auc: 0.9818 - val_loss: 0.0921 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m360/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8067 - auc: 0.9812 - loss: 0.0953\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8067 - auc: 0.9812 - loss: 0.0953 - val_accuracy: 0.8464 - val_auc: 0.9814 - val_loss: 0.0923 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8090 - auc: 0.9810 - loss: 0.0955 - val_accuracy: 0.8399 - val_auc: 0.9815 - val_loss: 0.0921 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8100 - auc: 0.9817 - loss: 0.0935 - val_accuracy: 0.8406 - val_auc: 0.9816 - val_loss: 0.0922 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8043 - auc: 0.9818 - loss: 0.0936 - val_accuracy: 0.8380 - val_auc: 0.9816 - val_loss: 0.0920 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8038 - auc: 0.9821 - loss: 0.0928 - val_accuracy: 0.8385 - val_auc: 0.9814 - val_loss: 0.0921 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m346/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8122 - auc: 0.9823 - loss: 0.0923\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8119 - auc: 0.9823 - loss: 0.0923 - val_accuracy: 0.8397 - val_auc: 0.9815 - val_loss: 0.0921 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8086 - auc: 0.9826 - loss: 0.0914 - val_accuracy: 0.8337 - val_auc: 0.9814 - val_loss: 0.0922 - learning_rate: 2.5000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8046 - auc: 0.9824 - loss: 0.0915 - val_accuracy: 0.8387 - val_auc: 0.9810 - val_loss: 0.0923 - learning_rate: 2.5000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8123 - auc: 0.9829 - loss: 0.0901 - val_accuracy: 0.8365 - val_auc: 0.9811 - val_loss: 0.0926 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8152 - auc: 0.9829 - loss: 0.0904 - val_accuracy: 0.8356 - val_auc: 0.9811 - val_loss: 0.0925 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m359/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8081 - auc: 0.9830 - loss: 0.0906\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8081 - auc: 0.9830 - loss: 0.0906 - val_accuracy: 0.8397 - val_auc: 0.9810 - val_loss: 0.0926 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8094 - auc: 0.9831 - loss: 0.0902 - val_accuracy: 0.8365 - val_auc: 0.9810 - val_loss: 0.0924 - learning_rate: 1.2500e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8103 - auc: 0.9839 - loss: 0.0891 - val_accuracy: 0.8340 - val_auc: 0.9809 - val_loss: 0.0925 - learning_rate: 1.2500e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8076 - auc: 0.9830 - loss: 0.0900 - val_accuracy: 0.8354 - val_auc: 0.9809 - val_loss: 0.0927 - learning_rate: 1.2500e-04\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "--------------------------------------------------\n",
      "Résultats pour Deep Learning (TensorFlow)\n",
      "--------------------------------------------------\n",
      "F1 Score (micro):     0.8446\n",
      "F1 Score (macro):     0.3816\n",
      "F1 Score (weighted):  0.8161\n",
      "Precision (micro):    0.8642\n",
      "Recall (micro):       0.8259\n",
      "Hamming Loss:         0.0328\n",
      "Subset Accuracy:      0.6894\n",
      "Modèle Deep Learning sauvegardé: /kaggle/working/models/deep_learning.keras\n",
      "\n",
      "------------------------------------------------------------\n",
      "RÉSUMÉ COMPARATIF DES MODÈLES\n",
      "------------------------------------------------------------\n",
      "                            f1_micro  f1_macro  f1_weighted  precision_micro  recall_micro  hamming_loss  subset_accuracy\n",
      "model                                                                                                                    \n",
      "Random Forest                 0.6917    0.4177       0.7917           0.5858        0.8443        0.0812           0.4891\n",
      "XGBoost                       0.8465    0.4513       0.8248           0.8630        0.8305        0.0325           0.6954\n",
      "CatBoost                      0.7136    0.4155       0.7910           0.6226        0.8358        0.0723           0.4737\n",
      "Deep Learning (TensorFlow)    0.8446    0.3816       0.8161           0.8642        0.8259        0.0328           0.6894\n",
      "\n",
      " Meilleur modèle (F1-micro): XGBoost\n",
      "Résultats sauvegardés: /kaggle/working/models/model_comparison.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    hamming_loss, accuracy_score\n",
    ")\n",
    "\n",
    "# XGBoost & CatBoost\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "PRODUCT_COLUMNS = [\n",
    "    'P5DA', 'RIBP', '8NN1', '7POT', '66FJ', 'GYSR', 'SOP4', 'RVSZ',\n",
    "    'PYUQ', 'LJR9', 'N2MW', 'AHXO', 'BSTQ', 'FM3X', 'K6QO', 'QBOL',\n",
    "    'JWFN', 'JZ9D', 'J9JW', 'GHYX', 'ECY3'\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MODELS_DIR = Path(\"/kaggle/working/models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTIONS D'ÉVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_multilabel(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Évalue les performances d'un modèle de classification multi-label.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"f1_micro\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"precision_micro\": precision_score(y_true, y_pred, average='micro'),\n",
    "        \"recall_micro\": recall_score(y_true, y_pred, average='micro'),\n",
    "        \"hamming_loss\": hamming_loss(y_true, y_pred),\n",
    "        \"subset_accuracy\": accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Résultats pour {model_name}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"F1 Score (micro):     {results['f1_micro']:.4f}\")\n",
    "    print(f\"F1 Score (macro):     {results['f1_macro']:.4f}\")\n",
    "    print(f\"F1 Score (weighted):  {results['f1_weighted']:.4f}\")\n",
    "    print(f\"Precision (micro):    {results['precision_micro']:.4f}\")\n",
    "    print(f\"Recall (micro):       {results['recall_micro']:.4f}\")\n",
    "    print(f\"Hamming Loss:         {results['hamming_loss']:.4f}\")\n",
    "    print(f\"Subset Accuracy:      {results['subset_accuracy']:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_model(model, name, encoder=None, scaler=None):\n",
    "    \"\"\"Sauvegarde un modèle et ses transformers.\"\"\"\n",
    "    model_path = MODELS_DIR / f\"{name}.pkl\"\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'encoder': encoder,\n",
    "            'scaler': scaler\n",
    "        }, f)\n",
    "\n",
    "    print(f\"Modèle sauvegardé: {model_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODÈLE 1: RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entraîne un Random Forest pour classification multi-label.\n",
    "    \"\"\"\n",
    "    print(\"\\n Entraînement Random Forest...\")\n",
    "\n",
    "    rf_base = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    model = MultiOutputClassifier(rf_base, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évaluation\n",
    "    results = evaluate_multilabel(y_test, y_pred, \"Random Forest\")\n",
    "\n",
    "    return model, results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODÈLE 2: XGBOOST\n",
    "# ============================================================================\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entraîne un XGBoost pour classification multi-label.\n",
    "    \"\"\"\n",
    "    print(\"\\n Entraînement XGBoost...\")\n",
    "\n",
    "    xgb_base = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "\n",
    "    model = MultiOutputClassifier(xgb_base, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évaluation\n",
    "    results = evaluate_multilabel(y_test, y_pred, \"XGBoost\")\n",
    "\n",
    "    return model, results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODÈLE 3: CATBOOST\n",
    "# ============================================================================\n",
    "\n",
    "def train_catboost(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entraîne un CatBoost pour classification multi-label.\n",
    "    \"\"\"\n",
    "    print(\"\\n Entraînement CatBoost...\")\n",
    "\n",
    "    cb_base = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=8,\n",
    "        learning_rate=0.1,\n",
    "        random_seed=RANDOM_STATE,\n",
    "        verbose=False,\n",
    "        auto_class_weights='Balanced'\n",
    "    )\n",
    "\n",
    "    model = MultiOutputClassifier(cb_base, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évaluation\n",
    "    results = evaluate_multilabel(y_test, y_pred, \"CatBoost\")\n",
    "\n",
    "    return model, results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODÈLE 4: DEEP LEARNING (TENSORFLOW)\n",
    "# ============================================================================\n",
    "\n",
    "def build_deep_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Construit un réseau de neurones pour classification multi-label.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Couche de sortie: sigmoid pour multi-label\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_deep_learning(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle de Deep Learning pour classification multi-label.\n",
    "    \"\"\"\n",
    "    print(\"\\n Entraînement Deep Learning (TensorFlow)...\")\n",
    "\n",
    "    # Conversion en arrays numpy\n",
    "    X_train_np = X_train.values.astype(np.float32)\n",
    "    X_test_np = X_test.values.astype(np.float32)\n",
    "    y_train_np = y_train.values.astype(np.float32)\n",
    "    y_test_np = y_test.values.astype(np.float32)\n",
    "\n",
    "    # Construction du modèle\n",
    "    model = build_deep_model(X_train_np.shape[1], y_train_np.shape[1])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Entraînement\n",
    "    history = model.fit(\n",
    "        X_train_np, y_train_np,\n",
    "        validation_data=(X_test_np, y_test_np),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Prédictions (seuil à 0.5)\n",
    "    y_pred_proba = model.predict(X_test_np)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Évaluation\n",
    "    results = evaluate_multilabel(y_test_np, y_pred, \"Deep Learning (TensorFlow)\")\n",
    "\n",
    "    return model, history, results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale pour entraîner tous les modèles.\n",
    "    \"\"\"\n",
    "    print(\"-\"*60)\n",
    "    print(\" ZIMNAT INSURANCE - CLASSIFICATION MULTI-LABEL\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Stockage des résultats\n",
    "    all_results = []\n",
    "\n",
    "    # 1. Random Forest\n",
    "    rf_model, rf_results = train_random_forest(X_train, y_train, X_test, y_test)\n",
    "    save_model(rf_model, \"random_forest\", encoder, scaler)\n",
    "    all_results.append(rf_results)\n",
    "\n",
    "    # 2. XGBoost\n",
    "    xgb_model, xgb_results = train_xgboost(X_train, y_train, X_test, y_test)\n",
    "    save_model(xgb_model, \"xgboost\", encoder, scaler)\n",
    "    all_results.append(xgb_results)\n",
    "\n",
    "    # 3. CatBoost\n",
    "    cb_model, cb_results = train_catboost(X_train, y_train, X_test, y_test)\n",
    "    save_model(cb_model, \"catboost\", encoder, scaler)\n",
    "    all_results.append(cb_results)\n",
    "\n",
    "    # 4. Deep Learning\n",
    "    dl_model, dl_history, dl_results = train_deep_learning(X_train, y_train, X_test, y_test)\n",
    "    dl_model.save(MODELS_DIR / \"deep_learning.keras\")\n",
    "    print(f\"Modèle Deep Learning sauvegardé: {MODELS_DIR / 'deep_learning.keras'}\")\n",
    "    all_results.append(dl_results)\n",
    "\n",
    "    # Résumé comparatif\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"RÉSUMÉ COMPARATIF DES MODÈLES\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df = results_df.set_index('model')\n",
    "    print(results_df.round(4).to_string())\n",
    "\n",
    "    # Meilleur modèle selon F1-micro\n",
    "    best_model = results_df['f1_micro'].idxmax()\n",
    "    print(f\"\\n Meilleur modèle (F1-micro): {best_model}\")\n",
    "\n",
    "    # Sauvegarder les résultats\n",
    "    results_df.to_csv(MODELS_DIR / \"model_comparison.csv\")\n",
    "    print(f\"Résultats sauvegardés: {MODELS_DIR / 'model_comparison.csv'}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba08512",
   "metadata": {
    "papermill": {
     "duration": 0.024608,
     "end_time": "2026-01-12T13:57:29.661562",
     "exception": false,
     "start_time": "2026-01-12T13:57:29.636954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9203311,
     "sourceId": 14475078,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 174.913366,
   "end_time": "2026-01-12T13:57:32.408199",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-12T13:54:37.494833",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
